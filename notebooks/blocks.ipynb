{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cezarygolecki/projects/magisterka/.venv/lib/python3.11/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/Users/cezarygolecki/projects/magisterka/.venv/lib/python3.11/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:268: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.insert(1, \"/\".join(os.path.abspath('').split(\"/\")[:-1]))\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformer.layers.multi_head_attention.attention_mechanism.attn_params import CosformerParams, VanillaParams\n",
    "from transformer.blocks import Block, HourglassBlock, TighteningBlock\n",
    "from transformer.blocks.utils import ShiftRight, DownsamplingLayer, UpsamplingLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hourglass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, max_length, d_model = 8, 32, 64\n",
    "num_heads = 4\n",
    "\n",
    "decoder = HourglassBlock(n_layers=[4,2,4],\n",
    "                         sizes=[32,16,32],\n",
    "                         d_model=d_model,\n",
    "                         num_heads=num_heads,\n",
    "                         method_params=CosformerParams(),\n",
    "                         apply_rotary_pos_enc=False,\n",
    "                         dropout=0.1,\n",
    "                         has_outproj=True,\n",
    "                         act_fun=None,\n",
    "                         post_norm= False,\n",
    "                         downsampling_type=\"avg\",\n",
    "                         upsampling_type=\"linear\",\n",
    "                         attention_downsampling= False,\n",
    "                         attention_upsampling= False,\n",
    "                         upsampling_residual= True,\n",
    "                         device='cpu'\n",
    "                         )\n",
    "\n",
    "x = torch.rand((batch_size, max_length, d_model))\n",
    "decoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "batch_size, max_length, d_model = 8, 32, 64\n",
    "num_heads = 4\n",
    "\n",
    "encoder = Block(n_layers=3,\n",
    "                  d_model=d_model,\n",
    "                  num_heads=num_heads,\n",
    "                  method_params=VanillaParams(),\n",
    "                  apply_rotary_pos_enc=True,\n",
    "                  dropout=0.1,\n",
    "                  has_outproj=True,\n",
    "                  act_fun=None,\n",
    "                  norm_before=True,\n",
    "                  device='cpu'\n",
    "                  )\n",
    "\n",
    "x = torch.rand((batch_size, max_length, d_model))\n",
    "encoder(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tightening Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, max_length, d_model = 8, 32, 64\n",
    "num_heads = 4\n",
    "\n",
    "tightening_block = TighteningBlock(n_layers=[2,2],\n",
    "                                   sizes=[max_length, int(max_length/2)],\n",
    "                          d_model=d_model,\n",
    "                          num_heads=num_heads,\n",
    "                          method_params=CosformerParams(),\n",
    "                          apply_rotary_pos_enc=True,\n",
    "                          dropout=0.1,\n",
    "                          has_outproj=True,\n",
    "                          act_fun=None,\n",
    "                          device='cpu'\n",
    "                          )\n",
    "\n",
    "x = torch.rand((batch_size, max_length, d_model))\n",
    "tightening_block(x).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
