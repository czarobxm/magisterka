{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cezarygolecki/projects/magisterka/.venv/lib/python3.11/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/Users/cezarygolecki/projects/magisterka/.venv/lib/python3.11/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:268: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.insert(1, \"/\".join(os.path.abspath('').split(\"/\")[:-1]))\n",
    "\n",
    "import torch\n",
    "from transformer.multi_head_attention.attention_mechanism import AttentionMechanism\n",
    "from transformer.multi_head_attention.attention_mechanism.attn_params import CosformerParams, PerformerParams, VanillaParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([n**2 for n in range(5, 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5]) torch.Size([2, 6, 5]) torch.Size([2, 6, 5])\n",
      "torch.Size([2, 3, 2])\n",
      "torch.Size([2, 6])\n",
      "tensor([[[     0,     30,     60,     90,    120],\n",
      "         [   400,    480,    560,    640,    720],\n",
      "         [  4300,   4730,   5160,   5590,   6020],\n",
      "         [  9075,   9680,  10285,  10890,  11495],\n",
      "         [ 26600,  27930,  29260,  30590,  31920],\n",
      "         [ 40750,  42380,  44010,  45640,  47270]],\n",
      "\n",
      "        [[ 81900,  84630,  87360,  90090,  92820],\n",
      "         [110425, 113580, 116735, 119890, 123045],\n",
      "         [185200, 189830, 194460, 199090, 203720],\n",
      "         [233100, 238280, 243460, 248640, 253820],\n",
      "         [351500, 358530, 365560, 372590, 379620],\n",
      "         [423775, 431480, 439185, 446890, 454595]]])\n"
     ]
    }
   ],
   "source": [
    "# DOWNSAMPLING\n",
    "from torch import nn\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 6\n",
    "factor = 2\n",
    "d_model = 5\n",
    "\n",
    "q = torch.arange(0, batch_size * seq_len//factor * d_model).reshape(batch_size, seq_len//factor, d_model)\n",
    "k = torch.arange(0, batch_size * seq_len * d_model).reshape(batch_size, seq_len, d_model)\n",
    "v = torch.arange(0, batch_size * seq_len * d_model).reshape(batch_size, seq_len, d_model)\n",
    "print(q.shape, k.shape, v.shape)\n",
    "k = k.view(\n",
    "            batch_size,\n",
    "            seq_len // factor,\n",
    "            factor,\n",
    "            d_model,\n",
    "            )\n",
    "weights = torch.einsum(\"bsd,bsfd->bsf\", q, k)\n",
    "print(weights.shape)\n",
    "weights = weights.flatten(1)\n",
    "print(weights.shape)\n",
    "out = torch.einsum(\"bs,bsd->bsd\", weights, v)\n",
    "print(out)\n",
    "avg_pool = nn.AvgPool1d(kernel_size=factor, stride=factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 5]) torch.Size([2, 3, 5]) torch.Size([2, 3, 5])\n",
      "tensor([[[  30,   80],\n",
      "         [ 430,  605],\n",
      "         [1330, 1630]],\n",
      "\n",
      "        [[2730, 3155],\n",
      "         [4630, 5180],\n",
      "         [7030, 7705]]])\n",
      "tensor([[[ 0,  1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8,  9],\n",
      "         [10, 11, 12, 13, 14]],\n",
      "\n",
      "        [[15, 16, 17, 18, 19],\n",
      "         [20, 21, 22, 23, 24],\n",
      "         [25, 26, 27, 28, 29]]])\n",
      "torch.Size([2, 6, 5])\n"
     ]
    }
   ],
   "source": [
    "# UPSAMPLING\n",
    "batch_size = 2\n",
    "seq_len = 6\n",
    "factor = 2\n",
    "d_model = 5\n",
    "\n",
    "q = torch.arange(0, batch_size * seq_len * d_model).reshape(batch_size, seq_len, d_model)\n",
    "k = torch.arange(0, batch_size * seq_len//factor * d_model).reshape(batch_size, seq_len//factor, d_model)\n",
    "v = torch.arange(0, batch_size * seq_len//factor * d_model).reshape(batch_size, seq_len//factor, d_model)\n",
    "print(q.shape, k.shape, v.shape)\n",
    "q = q.view(\n",
    "            batch_size,\n",
    "            seq_len // factor,\n",
    "            factor,\n",
    "            d_model,\n",
    "            )\n",
    "weights = torch.einsum(\"bsfd,bsd->bsf\", q, k)\n",
    "print(weights)\n",
    "print(v)\n",
    "a = torch.einsum(\"bsf,bsd->bsfd\", weights, v)\n",
    "print(a.view(batch_size, seq_len, d_model).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0,  1,  2,  3,  4],\n",
      "          [ 5,  6,  7,  8,  9]],\n",
      "\n",
      "         [[10, 11, 12, 13, 14],\n",
      "          [15, 16, 17, 18, 19]],\n",
      "\n",
      "         [[20, 21, 22, 23, 24],\n",
      "          [25, 26, 27, 28, 29]]],\n",
      "\n",
      "\n",
      "        [[[30, 31, 32, 33, 34],\n",
      "          [35, 36, 37, 38, 39]],\n",
      "\n",
      "         [[40, 41, 42, 43, 44],\n",
      "          [45, 46, 47, 48, 49]],\n",
      "\n",
      "         [[50, 51, 52, 53, 54],\n",
      "          [55, 56, 57, 58, 59]]]])\n",
      "tensor([[[ 0,  1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8,  9],\n",
      "         [10, 11, 12, 13, 14]],\n",
      "\n",
      "        [[15, 16, 17, 18, 19],\n",
      "         [20, 21, 22, 23, 24],\n",
      "         [25, 26, 27, 28, 29]]])\n",
      "tensor([[[  30,   80],\n",
      "         [ 430,  605],\n",
      "         [1330, 1630]],\n",
      "\n",
      "        [[2730, 3155],\n",
      "         [4630, 5180],\n",
      "         [7030, 7705]]])\n"
     ]
    }
   ],
   "source": [
    "# UPSAMPLING\n",
    "batch_size = 2\n",
    "seq_len = 6\n",
    "factor = 2\n",
    "d_model = 5\n",
    "\n",
    "q = torch.arange(0, batch_size * seq_len * d_model).reshape(batch_size, seq_len//factor, factor, d_model)\n",
    "k = torch.arange(0, batch_size * seq_len//factor * d_model).reshape(batch_size, seq_len//factor, d_model)\n",
    "\n",
    "print(q)\n",
    "print(k)\n",
    "\n",
    "weights = torch.einsum(\"bsfd,bsd->bsf\", q, k)\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 6\n",
    "n_heads = 2\n",
    "eps = 0.00000001\n",
    "query = key = value = torch.rand((batch_size, n_heads, seq_len, d_model//n_heads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0967, 0.4586, 0.5522, 0.7956, 0.8590, 0.3303, 0.9981, 0.4314],\n",
      "         [0.1161, 0.5292, 0.7440, 0.5752, 0.9253, 0.2856, 0.7630, 0.3724],\n",
      "         [0.2382, 0.3825, 0.8475, 0.7594, 0.7898, 0.2678, 0.6966, 0.5597],\n",
      "         [0.3342, 0.3822, 0.7424, 0.6594, 0.7903, 0.2991, 0.5103, 0.5450],\n",
      "         [0.4650, 0.4563, 0.7720, 0.6581, 0.7684, 0.3595, 0.5652, 0.4799],\n",
      "         [0.5582, 0.3749, 0.7021, 0.6443, 0.6653, 0.3863, 0.5724, 0.4100]],\n",
      "\n",
      "        [[0.5541, 0.1670, 0.8828, 0.0428, 0.3483, 0.3645, 0.4024, 0.8520],\n",
      "         [0.7369, 0.3503, 0.5351, 0.6196, 0.4813, 0.7096, 0.7302, 0.3852],\n",
      "         [0.7193, 0.3262, 0.6049, 0.4748, 0.5696, 0.6768, 0.6764, 0.4246],\n",
      "         [0.5083, 0.4979, 0.5489, 0.5324, 0.6331, 0.5941, 0.6304, 0.3710],\n",
      "         [0.6635, 0.4389, 0.5138, 0.5449, 0.5514, 0.6653, 0.6732, 0.3411],\n",
      "         [0.7344, 0.4274, 0.5199, 0.4742, 0.5061, 0.5453, 0.6727, 0.4627]]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 6\n",
    "embed_dim = 8\n",
    "device = \"cpu\"\n",
    "x = torch.rand((batch_size, seq_len, embed_dim)).to(device)\n",
    "v = torch.rand((batch_size, 200, embed_dim)).to(device)\n",
    "\n",
    "attn = AttentionMechanism(embed_dim, 4, VanillaParams(), False, device)\n",
    "print(attn(x,x,x, causal=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0389, 0.7063, 0.2557, 0.5169, 0.8304, 0.0929],\n",
      "         [0.2956, 0.5350, 0.5315, 0.4639, 0.7503, 0.1433],\n",
      "         [0.2882, 0.6009, 0.5320, 0.4246, 0.3359, 0.3572],\n",
      "         [0.4696, 0.4673, 0.5108, 0.6043, 0.5944, 0.4020],\n",
      "         [0.4350, 0.6297, 0.3256, 0.7654, 0.6061, 0.5014]]])\n",
      "torch.Size([1, 3, 5, 2])\n",
      "tensor([[[True, True, True, True, True, True],\n",
      "         [True, True, True, True, True, True],\n",
      "         [True, True, True, True, True, True],\n",
      "         [True, True, True, True, True, True],\n",
      "         [True, True, True, True, True, True]]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_len = 5\n",
    "embed_dim = 6\n",
    "num_heads = 3\n",
    "device = \"cpu\"\n",
    "\n",
    "from transformer.layers.multi_head_attention.attention_mechanism.cosformer import Cosformer\n",
    "\n",
    "x = torch.rand((batch_size, seq_len, embed_dim)).to(device)\n",
    "v = torch.rand((batch_size, seq_len, embed_dim)).to(device)\n",
    "\n",
    "attn = AttentionMechanism(embed_dim, num_heads, CosformerParams(), False, device)\n",
    "print(attn(x,v,v, causal=True))\n",
    "\n",
    "\n",
    "x = torch.rand((batch_size, num_heads, seq_len, embed_dim//num_heads)).to(device)\n",
    "v = torch.rand((batch_size, num_heads, seq_len, embed_dim//num_heads)).to(device)\n",
    "\n",
    "a = Cosformer(embed_dim, num_heads, 1e-8, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5286, 0.1363, 0.4103, 0.4354],\n",
       "         [0.7134, 0.5988, 0.6635, 1.0399],\n",
       "         [0.8281, 1.2339, 0.8651, 1.1266],\n",
       "         [1.4964, 2.3034, 1.3261, 1.5997],\n",
       "         [1.5555, 2.9737, 1.8209, 1.8884]],\n",
       "\n",
       "        [[0.1080, 0.3290, 0.3216, 0.0357],\n",
       "         [0.5594, 1.1571, 0.7290, 0.4924],\n",
       "         [1.0235, 1.5367, 0.7983, 0.5686],\n",
       "         [1.1612, 1.8700, 0.7925, 0.5744],\n",
       "         [2.1675, 2.4708, 1.0163, 1.0277]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 5\n",
    "embed_dim = 4\n",
    "device = \"cpu\"\n",
    "\n",
    "x = torch.rand((batch_size, seq_len, embed_dim)).to(device)\n",
    "v = torch.rand((batch_size,  seq_len, embed_dim)).to(device)\n",
    "\n",
    "attn = AttentionMechanism(embed_dim, 2, PerformerParams(),False, device)\n",
    "attn(x,x,v, causal=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
